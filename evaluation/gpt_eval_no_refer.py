import os
import sys
import logging
import json
import base64
from pathlib import Path
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import shutil
import openai
from datetime import datetime

# --- 1. é…ç½®æ—¥å¿— ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - [%(threadName)s] - %(message)s",
)
logger = logging.getLogger("ImageEvaluator")

# --- 2. åŠ è½½ç¯å¢ƒå˜é‡å’Œé…ç½® ---
load_dotenv()

# --- 3. é…ç½® OpenAI API å®¢æˆ·ç«¯ ---
try:
    client = openai.OpenAI(
        base_url=os.getenv('OPENAI_API_URL'),
        api_key=os.getenv('OPENAI_API_KEY'),
        timeout=float(os.getenv('OPENAI_TIMEOUT', 900.0)),
        max_retries=2,
    )
    API_MODEL = 'gpt-4o'
    logger.info(f"APIå®¢æˆ·ç«¯å·²é…ç½®ï¼Œä½¿ç”¨æ¨¡å‹: {API_MODEL}")
except Exception as e:
    logger.error(f"OpenAI API å®¢æˆ·ç«¯é…ç½®å¤±è´¥: {e}")
    sys.exit(1)

# --- 4. å›ºå®šè·¯å¾„é…ç½® ---
FIXED_INPUT_DIR = Path("./data2_resized_fix")#åŸå›¾ç‰‡è·¯å¾„
FIXED_PRED_IMAGES_BASE = Path("./edited_images_2_fix")#ç¼–è¾‘åå›¾ç‰‡è·¯å¾„
FIXED_OUTPUT_BASE = Path("./gpt_evaluation_4")#ä¿å­˜ç»“æœè·¯å¾„
FIXED_FAILED_BASE = Path("./gpt_evaluation_failed_4")#ä¿å­˜å¤±è´¥ç»“æœè·¯å¾„
FIXED_WORKERS = 20

# --- 5. è¾…åŠ©å‡½æ•° ---
def encode_image_to_base64(image_path: str) -> str:
    """å°†å›¾åƒç¼–ç ä¸ºbase64å­—ç¬¦ä¸²"""
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e:
        logger.error(f"ç¼–ç å›¾ç‰‡å¤±è´¥ {image_path}: {e}")
        return None

def scan_model_directories(pred_images_base: Path):
    """æ‰«æå¹¶è¿”å›æ‰€æœ‰æ¨¡å‹ç›®å½•"""
    model_dirs = []
    for item in sorted(pred_images_base.iterdir()):
        if item.is_dir() and not item.name.startswith('.'):
            model_dirs.append(item.name)
    return model_dirs

def scan_category_directories(input_dir: Path):
    """æ‰«æå¹¶è¿”å›æ‰€æœ‰ç±»åˆ«ç›®å½•"""
    category_dirs = []
    for item in sorted(input_dir.iterdir()):
        if item.is_dir() and not item.name.startswith('.'):
            category_dirs.append(item.name)
    return category_dirs

def select_from_list(items, item_type="ç›®å½•", allow_all=False):
    """é€šç”¨çš„äº¤äº’å¼é€‰æ‹©å‡½æ•°"""
    print("\n" + "=" * 50)
    print(f"æ£€æµ‹åˆ°ä»¥ä¸‹{item_type}:")
    print("=" * 50)
    if allow_all:
        print("0. [å…¨éƒ¨å¤„ç†]")
    for idx, item_name in enumerate(items, 1):
        print(f"{idx}. {item_name}")
    print("=" * 50)
    
    while True:
        try:
            choice = input(f"\nè¯·è¾“å…¥è¦å¤„ç†çš„{item_type}ç¼–å· (è¾“å…¥ 'q' é€€å‡º): ").strip()
            if choice.lower() == 'q':
                print("é€€å‡ºç¨‹åºã€‚")
                sys.exit(0)
            
            choice_idx = int(choice)
            
            if allow_all and choice_idx == 0:
                print(f"\nâœ“ å·²é€‰æ‹©: å…¨éƒ¨{item_type}\n")
                return None  # è¿”å› None è¡¨ç¤ºå…¨éƒ¨
            
            choice_idx -= 1
            if 0 <= choice_idx < len(items):
                selected = items[choice_idx]
                print(f"\nâœ“ å·²é€‰æ‹©: {selected}\n")
                return selected
            else:
                print(f"âŒ è¯·è¾“å…¥ {'0 åˆ°' if allow_all else '1 åˆ°'} {len(items)} ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("âŒ è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥æ•°å­—æˆ– 'q'")

def convert_number_to_filename(number_str: str) -> str:
    """å°†ä¸‰ä½æ•°å­—ç¼–å·è½¬æ¢ä¸ºæ–‡ä»¶åç”¨çš„ç¼–å·ï¼ˆå»é™¤å‰å¯¼é›¶ï¼‰"""
    return str(int(number_str))

# --- (å·²åˆ é™¤) check_task_scored å‡½æ•° ---
# (æ­¤å‡½æ•°å·²åˆ é™¤ï¼Œè·³è¿‡é€»è¾‘ç°åœ¨åœ¨ main å’Œ collect_tasks_for_category ä¸­)

# --- 6. è¯„ä¼°ç»´åº¦çš„ Prompt --- (å†…å®¹ä¸å˜)
PROMPTS = {
    "IF": """You are a professional image editing evaluation specialist.

You will receive for each sample:
- input_image: the original unedited image  
- pred_image: the candidate image produced by the model under test  
- prompt: the user editing instruction  
- editing_method: the specific operation type (text_delete | text_insert | text_change | text_correction | font_change | translation | position_shift | rotation | scaling)


Your job:
Evaluate **instruction_following (IF)** on a 0â€“5 scale. Focus only on whether the pred_image performs exactly the requested operation type (editing_method) and nothing else.  
Do **not** reward or penalize aspects unrelated to instruction compliance.

Penalty clause:  
If pred_image performs wrong operation type, multiple operations when only one requested, or fails to execute the specified operation, subtract points accordingly.

Scoring Scale (0â€“5):
5 = Exactly the specified operation, no extra changes  
4 = Correct operation with one tiny extra modification  
3 = Generally correct but some unnecessary additions/omissions  
2 = Operation attempted but with significant extra changes  
1 = Wrong operation type or major execution errors  
0 = No detectable change or completely wrong approach

Examples:
- Example 5: Instruction "Delete the word 'SALE'" with editing_method "text_delete" and pred_image shows only that word removed. Score 5.
- Example 3: Deleted "SALE" correctly but also changed font of other text when not requested. Score 3.
- Example 1: Used "text_change" (changed SALE to something else) instead of "text_delete", or deleted wrong text. Score 1.

Reasoning Steps:
1. Identify the requested operation from prompt and editing_method.  
2. Compare pred_image with input_image.  
3. Check if only the specified operation was performed.  
4. Assign score 0â€“5 using the scale and examples.

Output strictly in JSON:
{"IF": X, "rationale": "short explanation of why this score"}""",

    "TA": """You are an expert evaluator of text-in-image editing.

You will receive for each sample:
- input_image: the original unedited image
- pred_image: the candidate image produced by the model under test
- prompt: the user editing instruction


Your job:
Evaluate **text_accuracy (TA)** on a 0â€“5 scale. Focus only on whether the text content in pred_image matches the instruction.
Do **not** reward or penalize aspects unrelated to text accuracy (e.g., font style, layout, background).

Penalty clause:
If the text content is wrong, misspelled, mistranslated, or unchanged when should change, subtract points accordingly.

Scoring Scale (0â€“5):
5 = Target text fully correct in content/spelling/case
4 = Correct but with one minor spelling/formatting inconsistency
3 = Core text mostly correct but noticeable error remains
2 = Several text elements wrong or inconsistent
1 = Text largely incorrect or unchanged when should change
0 = Completely wrong text or no change

Examples:
- Example 5: Instruction "Change 'Sale' to 'Sold Out'" and pred_image shows exactly "Sold Out". Score 5.
- Example 4: Shows "Sold out" with lowercase 'o'. Score 4.
- Example 3: Shows "Sold Oat" instead of "Sold Out". Score 3.
- Example 1: Still shows "Sale" unchanged. Score 1.

Reasoning Steps:
1. Identify the requested text change from prompt.
2. Extract/compare text from input_image (before) and pred_image (after).
3. Check text content accuracy strictly against instruction.
4. Assign score 0â€“5 using the scale and examples.

Output strictly in JSON:
{"TA": X, "rationale": "short explanation of why this score"}""",

    "VC": """You are an expert evaluator of text-in-image editing.

You will receive for each sample:
- input_image: the original unedited image
- pred_image: the candidate image produced by the model under test
- prompt: the user editing instruction

Your job:
Evaluate **visual_consistency (VC)** on a 0â€“5 scale. Focus only on whether the new or edited elements visually integrate with the rest of the image in font/weight, alignment/perspective, edge anti-aliasing, color/lighting blending.
Do **not** reward or penalize unrelated content changes.

Penalty clause:
If new text or objects look pasted, misaligned, haloed, or style-mismatched, subtract points.

Scoring Scale (0â€“5):
5 = Perfect integration: matching font, size, color, alignment; no visible artifacts
4 = Very good integration: only tiny mismatch
3 = Moderate integration: visible mismatch (slight halo, mild misalignment)
2 = Poor integration: obvious halo or style mismatch
1 = Very poor integration: pasted look or misaligned
0 = Completely inconsistent/unreadable

Examples:
- Example 5: New text matches surrounding sign perfectly in font, color, and perspective. Score 5.
- Example 3: New text correct content but with mild white halo around edges. Score 3.
- Example 1: New text obviously pasted with wrong color and misaligned. Score 1.

Reasoning Steps:
1. Identify edited elements in pred_image.
2. Compare their visual integration quality to input_image.
3. Check for artifacts, misalignment, or style mismatches.
4. Assign score 0â€“5 using the scale and examples.

Output strictly in JSON:
{"VC": X, "rationale": "short explanation of why this score"}""",

    "LP": """You are an expert evaluator of text-in-image editing.

You will receive for each sample:
- input_image: the original unedited image
- pred_image: the candidate image produced by the model under test
- prompt: the user editing instruction

Your job:
Evaluate **layout_preservation (LP)** on a 0â€“5 scale. Focus only on whether non-target areas remained unchanged compared to input_image.
Do **not** reward or penalize the quality of instructed changes themselves.

Penalty clause:
If pred_image alters background, other objects or layout unnecessarily, subtract points.

Scoring Scale (0â€“5):
5 = All non-target areas unchanged
4 = Almost unchanged, only tiny disturbance
3 = Minor unrelated disturbance
2 = Several unrelated changes
1 = Large portions altered unnecessarily
0 = Layout completely different

Examples:
- Example 5: Only target text changed from "Open" to "Closed", rest of sign and background untouched. Score 5.
- Example 3: Target text changed correctly but background slightly shifted or blurred. Score 3.
- Example 1: New objects added and overall layout composition different. Score 1.

Reasoning Steps:
1. Identify target and non-target areas from prompt.
2. Compare pred_image with input_image.
3. Detect any unrelated changes to non-target areas.
4. Assign score 0â€“5 using the scale and examples.

Output strictly in JSON:
{"LP": X, "rationale": "short explanation of why this score"}""",

    "SE": """You are an expert evaluator of text-in-image editing.

You will receive for each sample:
- input_image: the original unedited image
- pred_image: the candidate image produced by the model under test
- prompt: the user editing instruction
- knowledge_prompt: semantic expectation or logical consequence

Your job:
Evaluate **semantic_expectation (SE)** on a 0â€“5 scale.
Use the knowledge_prompt to understand linked expectations and check whether these are reflected in pred_image.
Focus only on whether these linked expectations are satisfied.
Do **not** reward or penalize unrelated features.

Penalty clause:
If pred_image fails to reflect expected linked changes or contradicts knowledge_prompt, subtract points.

Scoring Scale (0â€“5):
5 = Fully matches all linked/knowledge expectations
4 = Matches but with one small linked detail missed
3 = Partially satisfies: core effect present but significant linked effect missing
2 = Several linked effects missing or inconsistent
1 = Contradicts or ignores knowledge expectation
0 = No evidence of linked effect

Examples:
- Example 5: Instruction "Remove chili from dish" + knowledge_prompt "Removing 'chili' implies spicy icon should be absent". Pred_image shows dish without chili and spicy icon gone. Score 5.
- Example 3: Chili removed correctly but spicy icon still present. Score 3.
- Example 1: Chili still present and spicy icon still present, ignoring both instruction and semantic expectation. Score 1.

Reasoning Steps:
1. Identify instruction and linked expectations from prompt + knowledge_prompt.
2. Compare pred_image with input_image.
3. Check whether linked expectations are satisfied beyond direct instruction.
4. Assign score 0â€“5 using the scale and examples.

Output strictly in JSON:
{"SE": X, "rationale": "short explanation of why this score"}"""
}

# --- 7. è°ƒç”¨ GPT-5 è¿›è¡Œè¯„ä¼° --- (å†…å®¹ä¸å˜)
# --- 7. è°ƒç”¨ GPT-5 è¿›è¡Œè¯„ä¼° --- (å·²ä¿®æ”¹)
def call_gpt5_for_evaluation(dimension: str, input_image_base64: str, pred_image_base64: str, 
                             prompt: str, editing_method: str = None, 
                             knowledge_prompt: str = None, retry_count: int = 3):
    """
    (å·²ä¿®æ”¹)
    è°ƒç”¨ GPT-5 è¿›è¡Œå•ä¸ªç»´åº¦çš„è¯„ä¼°
    - åˆ é™¤äº† output_image_base64 å‚æ•°
    """
    for attempt in range(retry_count):
        try:
            system_prompt = PROMPTS[dimension]
            
            # æ„å»ºç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_content = [
                {"type": "text", "text": f"User editing instruction (prompt): {prompt}"}
            ]
            
            if editing_method:
                user_content.append({"type": "text", "text": f"Editing method: {editing_method}"})
            
            if knowledge_prompt and dimension == "SE":
                user_content.append({"type": "text", "text": f"Knowledge prompt: {knowledge_prompt}"})
            
            # (å·²ä¿®æ”¹) æ·»åŠ å›¾ç‰‡ - åˆ é™¤äº† output_image
            user_content.extend([
                {"type": "text", "text": "Input image (original):"},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{input_image_base64}"}},
                {"type": "text", "text": "Pred image (model output):"},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{pred_image_base64}"}},
                
                # (å·²åˆ é™¤) Output image (expected reference)
                
                {"type": "text", "text": "\n**CRITICAL REQUIREMENT: Your entire response must be ONLY a valid JSON object in this exact format:**\n" + 
                                       f'{{\"{dimension}\": <score 0-5>, \"rationale\": \"<explanation>\"}}\n' +
                                       "**NO markdown, NO code blocks, NO explanations outside the JSON, NO think tags. Just the raw JSON object.**"}
            ])
            
            response = client.chat.completions.create(
                model=API_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                temperature=0.0,
                max_tokens=500
            )
            
            # ... (JSON æ¸…ç†å’Œè§£æé€»è¾‘ä¸å˜) ...
            result_text = response.choices[0].message.content.strip()
            
            import re
            result_text = result_text.replace('<think>', '').replace('</think>', '')
            result_text = result_text.replace('```json', '').replace('```', '')
            
            first_brace = result_text.find('{')
            if first_brace != -1:
                brace_count = 0
                json_end = -1
                for i in range(first_brace, len(result_text)):
                    if result_text[i] == '{':
                        brace_count += 1
                    elif result_text[i] == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            json_end = i + 1
                            break
                if json_end != -1:
                    result_text = result_text[first_brace:json_end]
            
            try:
                result_json = json.loads(result_text)
                if dimension in result_json and "rationale" in result_json:
                    return result_json
                else:
                    logger.warning(f"JSON ç¼ºå°‘å¿…è¦å­—æ®µï¼Œå°è¯•é‡è¯• ({attempt + 1}/{retry_count})")
                    logger.debug(f"è§£æçš„JSON: {result_json}")
            except json.JSONDecodeError as je:
                logger.warning(f"JSON è§£æå¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {result_text[:200]}")
                if attempt == retry_count - 1:
                    logger.error(f"æ— æ³•è§£æ GPT-5 è¿”å›çš„ JSON: {result_text}")
                    return {dimension: 0, "rationale": "JSON parsing failed"}
                
        except Exception as e:
            logger.error(f"è°ƒç”¨ GPT-5 è¯„ä¼° {dimension} æ—¶å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {e}")
            if attempt == retry_count - 1:
                return {dimension: 0, "rationale": f"API call failed: {str(e)}"}

# --- 8. è¯„ä¼°å•ä¸ªä»»åŠ¡ --- (å†…å®¹ä¸å˜)
# --- 8. è¯„ä¼°å•ä¸ªä»»åŠ¡ --- (å·²ä¿®æ”¹)
# --- 8. è¯„ä¼°å•ä¸ªä»»åŠ¡ --- (å·²ä¿®æ”¹)
def evaluate_task(task: dict, sample_dir: Path, pred_images_dir: Path):
    """
    (å·²ä¿®æ”¹)
    æ ¹æ®ä»»åŠ¡å†…å®¹ï¼Œè¯„ä¼°å„é¡¹æŒ‡æ ‡ï¼ˆIF, TA, VC, LP, SEï¼‰
    - ä¿®æ”¹ç‚¹: åˆ é™¤äº†æ‰€æœ‰ output_image (æ ‡å‡†ç­”æ¡ˆ) çš„é€»è¾‘
    """
    
    # --- 1. ä» task å­—å…¸ä¸­è·å–æ•°æ® ---
    
    task_id = task.get("id")
    prompt = task.get("prompt")
    editing_method = task.get("editing_method")
    knowledge_prompt = task.get("knowledge_prompt", None)

    # (æ–°å¢) ç›´æ¥ä» task ä¸­è·å–æ–‡ä»¶å
    input_image_filename = task.get("input_image")
    
    # (å·²åˆ é™¤) output_image_filename
    
    # (æ–°å¢) å¥å£®æ€§æ£€æŸ¥
    if not input_image_filename:
        raise ValueError(f"ä»»åŠ¡ {task_id} çš„ JSON ä¸­ç¼ºå°‘ 'input_image' é”®æˆ–å€¼ä¸ºç©º")
    
    # (å·²åˆ é™¤) output_image å¥å£®æ€§æ£€æŸ¥

    
    # --- 2. (å·²ä¿®æ”¹) æ‹¼æ¥è·¯å¾„ ---
    
    # (å·²ä¿®æ”¹) 1. input_image è·¯å¾„: (ä½¿ç”¨ task ä¸­çš„ "input_image" å€¼)
    input_image_path = sample_dir / input_image_filename
    
    # (å·²åˆ é™¤) 2. output_image è·¯å¾„
    
    # 3. pred_image è·¯å¾„: (æ­¤é€»è¾‘ä¿æŒä¸å˜)
    sample_number = sample_dir.name
    pred_image_path = pred_images_dir / f"{sample_number}_edited.jpg"
    
    
    # --- 3. (å·²ä¿®æ”¹) æ£€æŸ¥ã€ç¼–ç å’Œè¯„ä¼° ---
    
    # (å·²ä¿®æ”¹) æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ - åˆ é™¤äº† output
    for img_path, img_type in [(input_image_path, "input"), (pred_image_path, "pred")]:
        if not img_path.exists():
            raise FileNotFoundError(f"{img_type} å›¾ç‰‡æœªæ‰¾åˆ°: {img_path}")
    
    # (å·²ä¿®æ”¹) ç¼–ç å›¾ç‰‡ä¸º base64 - åˆ é™¤äº† output
    input_image_base64 = encode_image_to_base64(str(input_image_path))
    pred_image_base64 = encode_image_to_base64(str(pred_image_path))
    
    # (å·²ä¿®æ”¹) æ£€æŸ¥ç¼–ç  - åˆ é™¤äº† output
    if not all([input_image_base64, pred_image_base64]):
        raise ValueError("å›¾ç‰‡ç¼–ç å¤±è´¥")
    
    # (å·²ä¿®æ”¹) è¯„ä¼°å„ä¸ªç»´åº¦ - åˆ é™¤äº† output_image_base64 å‚æ•°
    evaluation_results = {}
    
    logger.info(f"[{task_id}] å¼€å§‹è¯„ä¼° IF...")
    evaluation_results["IF"] = call_gpt5_for_evaluation(
        "IF", input_image_base64, pred_image_base64, 
        prompt, editing_method
    )
    
    logger.info(f"[{task_id}] å¼€å§‹è¯„ä¼° TA...")
    evaluation_results["TA"] = call_gpt5_for_evaluation(
        "TA", input_image_base64, pred_image_base64, prompt
    )
    
    logger.info(f"[{task_id}] å¼€å§‹è¯„ä¼° VC...")
    evaluation_results["VC"] = call_gpt5_for_evaluation(
        "VC", input_image_base64, pred_image_base64, prompt
    )
    
    logger.info(f"[{task_id}] å¼€å§‹è¯„ä¼° LP...")
    evaluation_results["LP"] = call_gpt5_for_evaluation(
        "LP", input_image_base64, pred_image_base64, prompt
    )
    
    # å¦‚æœæœ‰ knowledge_promptï¼Œè¯„ä¼° SE
    if knowledge_prompt:
        logger.info(f"[{task_id}] å¼€å§‹è¯„ä¼° SE...")
        evaluation_results["SE"] = call_gpt5_for_evaluation(
            "SE", input_image_base64, pred_image_base64, 
            prompt, knowledge_prompt=knowledge_prompt
        )
    
    return evaluation_results
# --- 9. å¤„ç†å•ä¸ªä»»åŠ¡ --- (å†…å®¹ä¸å˜)
def process_single_task(task: dict, sample_dir: Path, pred_images_dir: Path, output_dir: Path, failed_dir: Path):
    """å¤„ç†å•ä¸ªä»»åŠ¡ï¼Œä¸å†å†™å…¥æ–‡ä»¶ï¼Œè€Œæ˜¯è¿”å›ç»“æœå­—å…¸æˆ–å¤±è´¥å­—å…¸"""
    task_id = task.get("id")
    
    try:
        evaluation_results = evaluate_task(task, sample_dir, pred_images_dir)
        
        result = {
            "task_id": task_id,
            "evaluation_results": evaluation_results
        }
        
        logger.info(f"âœ“ [{task_id}] è¯„ä¼°å®Œæˆ")
        return {"status": "success", "data": result}
        
    except Exception as e:
        logger.error(f"âœ— [{task_id}] å¤„ç†å¤±è´¥: {e}")
        return {"status": "error", "task_id": task_id, "error": str(e)}

# --- 10. (æ–°å¢) æ”¶é›†å•ä¸ªç±»åˆ«ç›®å½•ä¸­çš„ä»»åŠ¡ ---
def collect_tasks_for_category(input_dir: Path, selected_category: str, pred_images_dir: Path, 
                             already_scored_set: set):
    """
    (æ–°å¢)
    æ‰«æå•ä¸ªç±»åˆ«ç›®å½•ï¼Œè§£æJSONï¼Œè¿‡æ»¤å·²è¯„æµ‹ä»»åŠ¡ï¼Œå¹¶è¿”å›å¾…è¿è¡Œçš„ä»»åŠ¡åˆ—è¡¨ã€‚
    
    è¿”å›: 
        (tasks_to_run, skipped_tasks_count)
        tasks_to_run: [(task_dict, sample_dir_path), ...]
    """
    
    logger.info(f"--- æ­£åœ¨æ‰«æç±»åˆ«: {selected_category} ---")
    
    tasks_to_run = []
    skipped_tasks = 0
    
    category_dir = input_dir / selected_category
    
    if not category_dir.is_dir():
        logger.error(f"é€‰å®šçš„ç±»åˆ«ç›®å½•ä¸å­˜åœ¨: {category_dir}")
        return [], 0
    
    if not pred_images_dir.is_dir():
        logger.warning(f"predå›¾ç‰‡ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡ç±»åˆ«: {pred_images_dir}")
        return [], 0
    
    # éå†æ ·æœ¬ç›®å½• (001, 002, 003 ç­‰)
    for sample_dir in sorted(category_dir.iterdir()):
        if not sample_dir.is_dir() or sample_dir.name.startswith('.'):
            continue
        
        # --- (JSON æŸ¥æ‰¾é€»è¾‘ - ä¿æŒä¸å˜) ---
        sample_number = sample_dir.name
        stripped_number = convert_number_to_filename(sample_number)
        suffixes_to_check = [
            f"_{sample_number}.json", f".{sample_number}.json",
            f"-{sample_number}.json", f"{sample_number}.json",
        ]
        if stripped_number != sample_number:
            suffixes_to_check.extend([
                f"_{stripped_number}.json", f".{stripped_number}.json",
                f"-{stripped_number}.json", f"{stripped_number}.json",
            ])
        
        json_path = None
        all_files_in_dir = [f for f in sample_dir.iterdir() if f.is_file()]
        found = False
        for suffix in suffixes_to_check:
            for file_path in all_files_in_dir:
                file_name = file_path.name
                if file_name.endswith(suffix):
                    if file_name == suffix:
                        json_path = file_path
                        logger.debug(f"æ‰¾åˆ°JSONæ–‡ä»¶ (å®Œå…¨åŒ¹é…): {file_name}")
                        found = True
                        break
                    separator_index = len(file_name) - len(suffix)
                    if file_name[separator_index] == suffix[0]: 
                        json_path = file_path
                        logger.debug(f"æ‰¾åˆ°JSONæ–‡ä»¶ (åç¼€åŒ¹é…): {file_name}")
                        found = True
                        break
            if found:
                break
        # --- (JSON æŸ¥æ‰¾é€»è¾‘ç»“æŸ) ---

        if json_path is None:
            logger.warning(f"åœ¨ {sample_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•JSONæ–‡ä»¶ï¼Œå·²å°è¯•åç¼€: {suffixes_to_check}")
            continue

        try:
            # --- (JSON è¯»å–ã€ä¿®å¤ã€è§£æé€»è¾‘ - ä¿æŒä¸å˜) ---
            with open(json_path, "r", encoding="utf-8-sig") as f:
                content = f.read()
            content = content.strip()
            if content.endswith(",}"): content = content[:-2] + "}"
            if content.endswith(",]"): content = content[:-2] + "]"
            
            try: data = json.loads(content)
            except json.JSONDecodeError as e:
                logger.error(f"[{selected_category}/{sample_number}] JSON æ ¼å¼é”™è¯¯: {e}")
                continue
            
            if isinstance(data, dict): tasks = data.get("tasks", [data])
            elif isinstance(data, list): tasks = data
            elif isinstance(data, str):
                try:
                    tasks = json.loads(data)
                    if isinstance(tasks, dict): tasks = [tasks]
                except:
                    logger.error(f"[{selected_category}/{sample_number}] æ— æ³•è§£æå­—ç¬¦ä¸² JSON")
                    continue
            else:
                logger.error(f"[{selected_category}/{sample_number}] ä¸æ”¯æŒçš„ JSON æ ¼å¼: {type(data)}")
                continue
            if not isinstance(tasks, list): tasks = [tasks]
            # --- (JSON è§£æç»“æŸ) ---
            
            logger.info(f"[{selected_category}/{sample_number}] æ‰¾åˆ° {len(tasks)} ä¸ªä»»åŠ¡")
            
            new_tasks_in_file = 0
            # (å·²ä¿®æ”¹) è¿‡æ»¤ä»»åŠ¡
            for task in tasks:
                if not isinstance(task, dict):
                    logger.error(f"è·³è¿‡éå­—å…¸ä»»åŠ¡: {type(task)}")
                    continue
                
                task_id = task.get("id", "")
                if not task_id:
                    logger.error(f"ä»»åŠ¡ç¼ºå°‘ id å­—æ®µï¼Œè·³è¿‡")
                    continue
                
                # (å·²ä¿®æ”¹) æ ¸å¿ƒè·³è¿‡é€»è¾‘
                task_key = (selected_category, task_id)
                
                if task_key in already_scored_set:
                    # logger.info(f"[{selected_category}/{sample_number}] SKIPPING å·²è¯„æµ‹ä»»åŠ¡: {task_id}") # (æ³¨é‡Šæ‰ä»¥å‡å°‘æ—¥å¿—)
                    skipped_tasks += 1
                    continue
                    
                # (å·²ä¿®æ”¹) æ·»åŠ åˆ°å¾…è¿è¡Œåˆ—è¡¨
                tasks_to_run.append((task, sample_dir)) 
                new_tasks_in_file += 1
            
            if new_tasks_in_file == 0 and len(tasks) > 0:
                logger.info(f"[{selected_category}/{sample_number}] æ‰€æœ‰ä»»åŠ¡å·²è¯„æµ‹ï¼Œè·³è¿‡")
            elif new_tasks_in_file > 0:
                logger.info(f"[{selected_category}/{sample_number}] æ–°å¢ {new_tasks_in_file} ä¸ªå¾…è¯„åˆ†ä»»åŠ¡")
                
        except Exception as e:
            logger.error(f"è¯»å– JSON æ–‡ä»¶å¤±è´¥: {json_path}, é”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    logger.info(f"--- ç±»åˆ« [{selected_category}] æ‰«æå®Œæˆ: æ‰¾åˆ° {len(tasks_to_run)} ä¸ªæ–°ä»»åŠ¡, è·³è¿‡ {skipped_tasks} ä¸ª ---")
    return tasks_to_run, skipped_tasks


# --- 11. ä¸»å‡½æ•° --- (å·²ä¿®æ”¹)
def main():

    # å…¨å±€ç»Ÿè®¡ (ç”¨äºæœ€åçš„æ€»ç»“)
    total_all = 0
    success_all = 0
    failed_all = 0
    skipped_all = 0
    
    # --- è·¯å¾„æ£€æŸ¥ (ä¸å˜) ---
    if not FIXED_INPUT_DIR.is_dir():
        logger.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {FIXED_INPUT_DIR}")
        sys.exit(1)
    if not FIXED_PRED_IMAGES_BASE.is_dir():
        logger.error(f"predå›¾ç‰‡åŸºç¡€ç›®å½•ä¸å­˜åœ¨: {FIXED_PRED_IMAGES_BASE}")
        sys.exit(1)
    FIXED_OUTPUT_BASE.mkdir(parents=True, exist_ok=True)
    FIXED_FAILED_BASE.mkdir(parents=True, exist_ok=True)
    
    
    # --- 1. æ‰«æå¹¶é€‰æ‹©æ¨¡å‹ (ä¸å˜) ---
    model_dirs = scan_model_directories(FIXED_PRED_IMAGES_BASE)
    print(model_dirs)
    if not model_dirs:
        logger.error(f"åœ¨ {FIXED_PRED_IMAGES_BASE} ä¸‹æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹ç›®å½•")
        sys.exit(1)
    selected_model = select_from_list(model_dirs, "æ¨¡å‹", allow_all=True)
    
    if selected_model is None:
        models_to_process = model_dirs
        logger.info(f"\nå°†å¤„ç†æ‰€æœ‰ {len(models_to_process)} ä¸ªæ¨¡å‹")
    else:
        models_to_process = [selected_model]
    
    # --- 2. æ‰«æå¹¶é€‰æ‹©ç±»åˆ« (ä¸å˜) ---
    category_dirs = scan_category_directories(FIXED_INPUT_DIR)
    if not category_dirs:
        logger.error(f"åœ¨ {FIXED_INPUT_DIR} ä¸‹æœªæ‰¾åˆ°ä»»ä½•ç±»åˆ«ç›®å½•")
        sys.exit(1)
    selected_category = select_from_list(category_dirs, "ç±»åˆ«", allow_all=True)
    
    if selected_category is None:
        categories_to_process = category_dirs
        logger.info(f"\nå°†å¤„ç†æ‰€æœ‰ {len(categories_to_process)} ä¸ªç±»åˆ«")
    else:
        categories_to_process = [selected_category]
    
    # ==================================================================
    # --- 4. (å·²é‡æ„) æ‰¹é‡å¤„ç† ---
    # ==================================================================
    for model in models_to_process:
        
        logger.info(f"\n{'#'*60}")
        logger.info(f"å¼€å§‹å¤„ç†æ¨¡å‹: {model}")
        logger.info(f"{'#'*60}\n")
        
        # --- (æŒ‰æ¨¡å‹åŠ è½½/ä¿å­˜é€»è¾‘ - ä¿æŒä¸å˜) ---
        results_filename = FIXED_OUTPUT_BASE / f"{model}.json"
        failed_filename = FIXED_FAILED_BASE / f"{model}.json"
        
        model_results = []
        model_failures = []
        model_scored_set = set() # (category, task_id)
        
        if results_filename.exists():
            logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹ [{model}] çš„å·²æœ‰ç»“æœ: {results_filename}")
            try:
                with open(results_filename, "r", encoding="utf-8") as f:
                    model_results = json.load(f)
                    if not isinstance(model_results, list): model_results = []
                
                for item in model_results:
                    if isinstance(item, dict):
                        key = (item.get('category'), item.get('task_id'))
                        if all(key): model_scored_set.add(key)
                logger.info(f"æˆåŠŸåŠ è½½ {len(model_results)} æ¡ [{model}] çš„å·²æœ‰è¯„ä¼°ã€‚")
            except Exception as e:
                logger.error(f"åŠ è½½ {results_filename} å¤±è´¥: {e}ã€‚å°†ä½œä¸ºæ–°è¿è¡Œå¼€å§‹ã€‚")
                model_results = []; model_scored_set = set()
        else:
            logger.info(f"æœªæ‰¾åˆ° [{model}] çš„æ—§ç»“æœï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶: {results_filename}")

        if failed_filename.exists():
            try:
                with open(failed_filename, "r", encoding="utf-8") as f:
                    model_failures = json.load(f)
                    if not isinstance(model_failures, list): model_failures = []
            except Exception as e:
                logger.error(f"åŠ è½½ {failed_filename} å¤±è´¥: {e}ã€‚")
                model_failures = []
        
        pred_images_base_for_model = FIXED_PRED_IMAGES_BASE / model
        
        # --- (æ­¤æ¨¡å‹çš„ç»Ÿè®¡ - ä¿æŒä¸å˜) ---
        total_model_new = 0
        success_model_new = 0
        failed_model_new = 0
        skipped_model_new = 0
        
        # ==================================================================
        # (å·²ä¿®æ”¹) æ ¸å¿ƒé€»è¾‘å˜æ›´:
        # 1. å…ˆæ”¶é›†æ‰€æœ‰ç±»åˆ«çš„ä»»åŠ¡
        # 2. å†å°†æ‰€æœ‰ä»»åŠ¡æ”¾å…¥ä¸€ä¸ªçº¿ç¨‹æ± 
        # ==================================================================
        
        logger.info(f"--- æ­£åœ¨ä¸ºæ¨¡å‹ [{model}] æ”¶é›†æ‰€æœ‰ç±»åˆ«çš„ä»»åŠ¡... ---")
        
        all_tasks_to_submit = [] # æ ¼å¼: (task, sample_dir, pred_images_dir, category)
        
        for category in categories_to_process:
            pred_images_dir_for_cat = pred_images_base_for_model / category
            
            # (å·²ä¿®æ”¹) è°ƒç”¨æ–°å‡½æ•°
            tasks_for_this_cat, skipped_in_cat = collect_tasks_for_category(
                input_dir = FIXED_INPUT_DIR,
                selected_category = category,
                pred_images_dir = pred_images_dir_for_cat,
                already_scored_set = model_scored_set
            )
            
            skipped_model_new += skipped_in_cat
            
            # (å·²ä¿®æ”¹) å‡†å¤‡æäº¤åˆ—è¡¨
            for task, sample_dir in tasks_for_this_cat:
                all_tasks_to_submit.append((task, sample_dir, pred_images_dir_for_cat, category))
        
        total_model_new = len(all_tasks_to_submit)
        
        if total_model_new == 0:
            logger.info(f"æ¨¡å‹ [{model}] æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–°ä»»åŠ¡ (å·²è·³è¿‡ {skipped_model_new} ä¸ªå·²è¯„æµ‹ä»»åŠ¡)ã€‚")
            logger.info(f"{'#'*60}\n")
            continue # (æ–°å¢) è·³è¿‡æ­¤æ¨¡å‹
        
        logger.info(f"--- æ¨¡å‹ [{model}] å…±æ‰¾åˆ° {total_model_new} ä¸ªæ–°ä»»åŠ¡ (è·³è¿‡ {skipped_model_new} ä¸ªå·²è¯„æµ‹)ï¼Œå¼€å§‹å¹¶å‘å¤„ç†... ---")
        
        # (å·²ä¿®æ”¹) ä½¿ç”¨ä¸€ä¸ªå¤§çš„çº¿ç¨‹æ± å¤„ç†æ‰€æœ‰ä»»åŠ¡
        with ThreadPoolExecutor(max_workers=FIXED_WORKERS, thread_name_prefix=f'{model}-Evaluator') as executor:
            
            # (å·²ä¿®æ”¹) æäº¤ä»»åŠ¡å¹¶å­˜å‚¨ä¸Šä¸‹æ–‡
            futures = {}
            for task, sample_dir, pred_images_dir, category in all_tasks_to_submit:
                future = executor.submit(process_single_task, task, sample_dir, pred_images_dir, FIXED_OUTPUT_BASE, FIXED_FAILED_BASE)
                futures[future] = (task, category) # å­˜å‚¨ä¸Šä¸‹æ–‡ (task, category)
            
            # (å·²ä¿®æ”¹) æ”¶é›†ç»“æœ
            for future in tqdm(as_completed(futures), total=len(futures), desc=f"Evaluating {model}"):
                task_obj, category = futures[future] # è·å–ä¸Šä¸‹æ–‡
                task_id = task_obj.get("id", "Unknown ID")
                
                try:
                    result_object = future.result()
                    
                    if result_object.get("status") == "success":
                        res_data = result_object["data"]
                        res_data["model"] = model
                        res_data["category"] = category
                        model_results.append(res_data) # (ä¿®æ”¹) è¿½åŠ åˆ° model_results
                        success_model_new += 1
                    else:
                        fail_data = result_object
                        fail_data["model"] = model
                        fail_data["category"] = category
                        model_failures.append(fail_data) # (ä¿®æ”¹) è¿½åŠ åˆ° model_failures
                        failed_model_new += 1
                        
                except Exception as e:
                    # æ•è·çº¿ç¨‹æ‰§è¡Œä¸­çš„å¼‚å¸¸
                    logger.error(f"ä»»åŠ¡ {task_id} æ‰§è¡Œå¼‚å¸¸: {e}")
                    model_failures.append({
                        "status": "error", 
                        "task_id": task_id, 
                        "model": model,
                        "category": category,
                        "error": f"Future execution error: {str(e)}"
                    })
                    failed_model_new += 1

        # ==================================================================
        # (é€»è¾‘ä¿®æ”¹ç»“æŸ)
        # ==================================================================

        # --- (ä¿å­˜å’Œæ—¥å¿—è®°å½•é€»è¾‘ - ä¿æŒä¸å˜) ---
        logger.info(f"\n{'#'*60}")
        logger.info(f"æ¨¡å‹ [{model}] çš„æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼Œæ­£åœ¨ä¿å­˜æ–‡ä»¶...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if results_filename.exists():
            try:
                backup_name = results_filename.with_suffix(f".{timestamp}.bak.json")
                shutil.copy(results_filename, backup_name)
                logger.info(f"å·²å¤‡ä»½æ—§çš„ {results_filename.name} åˆ°: {backup_name.name}")
            except Exception as e:
                logger.warning(f"å¤‡ä»½ {results_filename.name} å¤±è´¥: {e}")

        if failed_filename.exists():
             try:
                backup_name = failed_filename.with_suffix(f".{timestamp}.bak.json")
                shutil.copy(failed_filename, backup_name)
                logger.info(f"å·²å¤‡ä»½æ—§çš„ {failed_filename.name} åˆ°: {backup_name.name}")
             except Exception as e:
                logger.warning(f"å¤‡ä»½ {failed_filename.name} å¤±è´¥: {e}")

        try:
            with open(results_filename, "w", encoding="utf-8") as f:
                json.dump(model_results, f, indent=4, ensure_ascii=False)
            logger.info(f"âœ“ [{model}] æ€»å…± {len(model_results)} æ¡æˆåŠŸç»“æœå·²ä¿å­˜åˆ°: {results_filename}")
        except Exception as e:
            logger.error(f"âœ— ä¿å­˜ [{model}] ç»“æœæ–‡ä»¶å¤±è´¥: {e}")

        try:
            with open(failed_filename, "w", encoding="utf-8") as f:
                json.dump(model_failures, f, indent=4, ensure_ascii=False)
            logger.info(f"âœ“ [{model}] æ€»å…± {len(model_failures)} æ¡å¤±è´¥è®°å½•å·²ä¿å­˜åˆ°: {failed_filename}")
        except Exception as e:
            logger.error(f"âœ— ä¿å­˜ [{model}] å¤±è´¥æ–‡ä»¶å¤±è´¥: {e}")

        logger.info(f"\n--- æ¨¡å‹ [{model}] æœ¬æ¬¡è¿è¡Œç»Ÿè®¡ ---")
        logger.info(f" Â æ–°å¢ä»»åŠ¡ (æœ¬æ¬¡è¿è¡Œ): {total_model_new}")
        logger.info(f" Â â””â”€ æˆåŠŸ: {success_model_new}")
        logger.info(f" Â â””â”€ å¤±è´¥: {failed_model_new}")
        logger.info(f" Â è·³è¿‡ä»»åŠ¡ (å·²è¯„æµ‹): {skipped_model_new}")
        logger.info(f" Â --- ç´¯è®¡æ€»æ•° (åœ¨æ–‡ä»¶ä¸­) ---")
        logger.info(f" Â ç´¯è®¡æˆåŠŸ: {len(model_results)}")
        logger.info(f" Â ç´¯è®¡å¤±è´¥: {len(model_failures)}")
        logger.info(f"{'#'*60}\n")
        
        # ç´¯åŠ åˆ°å…¨å±€ç»Ÿè®¡
        total_all += total_model_new
        success_all += success_model_new
        failed_all += failed_model_new
        skipped_all += skipped_model_new

    # --- 5. (å…¨å±€ç»Ÿè®¡æ—¥å¿— - ä¿æŒä¸å˜) ---
    logger.info("\n" + "=" * 60)
    logger.info(" ğŸ‰ æ‰€æœ‰é€‰å®šæ¨¡å‹çš„æ‰¹é‡ä»»åŠ¡å®Œæˆ ")
    logger.info("=" * 60)
    logger.info(" å…¨å±€ç»Ÿè®¡æ‘˜è¦ (æœ¬æ¬¡è¿è¡Œ) ")
    logger.info("=" * 60)
    logger.info(f"å¤„ç†æ¨¡å‹æ•°: {len(models_to_process)}")
    logger.info(f"å¤„ç†ç±»åˆ«æ•°: {len(categories_to_process)}")
    logger.info(f"æ€»è®¡æ–°å¢ä»»åŠ¡ (æ‰€æœ‰æ¨¡å‹): {total_all}")
    logger.info(f"â””â”€ æˆåŠŸ: {success_all}")
    logger.info(f"â””â”€ å¤±è´¥: {failed_all}")
    logger.info(f"æ€»è®¡è·³è¿‡ä»»åŠ¡ (æ‰€æœ‰æ¨¡å‹): {skipped_all}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()